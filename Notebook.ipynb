{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns \n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,  cross_val_score\n",
    "from sklearn import preprocessing, linear_model\n",
    "from sklearn.preprocessing import  LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler \n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "\n",
    "df = pd.read_csv('data.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate entries\n",
    "\n",
    "print(\"Duplicate entry in data:\",len(df[df.duplicated()])) \n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datainfo(df):\n",
    "    temp_ps = pd.DataFrame(index=df.columns)\n",
    "    temp_ps['DataType'] = df.dtypes\n",
    "    temp_ps[\"Count\"] = df.count()\n",
    "    temp_ps['Unique values'] = df.nunique()\n",
    "    temp_ps['Missing values'] = df.isnull().sum()\n",
    "    temp_ps['Missing values percentage'] = (temp_ps['Missing values']/len(df))*100 \n",
    "    df_desc = df.describe().transpose()\n",
    "    temp_ps['Count'] = df_desc['count']\n",
    "    temp_ps['Mean'] = df_desc['mean']\n",
    "    temp_ps['Std'] = df_desc['std']\n",
    "    temp_ps['Min'] = df_desc['min']\n",
    "    temp_ps['25%'] = df_desc['25%']\n",
    "    temp_ps['50%'] = df_desc['50%']\n",
    "    temp_ps['75%'] = df_desc['75%']\n",
    "    temp_ps['Max'] = df_desc['max']\n",
    "    numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    temp_ps['Skewness'] = df[numerical_columns].skew()\n",
    "    temp_ps['Kurtosis'] = df[numerical_columns].kurtosis()\n",
    "    return temp_ps\n",
    "\n",
    "display(datainfo(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see 4 categorical variables\n",
    "- We can see 10 discrete variables\n",
    "\n",
    "- No missing values \n",
    "- Low unique counts in last 3 columns\n",
    "\n",
    "- Very heavy skewness on Rainfall and Snowfall will have to be treated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.groupby('Functioning Day').sum()['Rented Bike Count'].sort_values(ascending = False).reset_index())\n",
    "\n",
    "# We only have data for Functioning Day = Yes, so we can drop this column and the rows where Functioning Day = No\n",
    "before = len(df)\n",
    "df_filtered=df.drop(df[df['Functioning Day'] == 'No'].index) \n",
    "df_filtered.drop(['Functioning Day'], axis=1, inplace = True)\n",
    "\n",
    "# Number of rows remaining\n",
    "print(\"Number of rows remaining:\", len(df_filtered), f\"({round(len(df_filtered)/before*100,2)}% of the original dataset)\")\n",
    "\n",
    "#TODO: A justifier par une visualisation\n",
    "colors = [\"green\", \"purple\"]\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.catplot(x='Hour', y='Rented Bike Count', hue='Functioning Day', data=df, kind='point', height=5, aspect=2, palette=colors)\n",
    "\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Rented Bike Count')\n",
    "plt.title('Hourly Distribution of Rented Bike Count based on Functioning Day')\n",
    "\n",
    "legend_labels = ['Non-Working Day', 'Working Day']\n",
    "legend_handles = [plt.Line2D([0], [0], marker='o', color=colors[i], label=label, markersize=5) for i, label in enumerate(legend_labels)]\n",
    "plt.legend(handles=legend_handles, title='Functioning Day')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# We can see that all non-working days are always at 0, so we can drop this column and the rows where Functioning Day = No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the date column into day, month and year columns\n",
    "\n",
    "df_filtered['Date'] = pd.to_datetime(df_filtered['Date'], format='%d/%m/%Y')\n",
    "df_filtered['Day'] = df_filtered['Date'].dt.day\n",
    "df_filtered['Month'] = df_filtered['Date'].dt.month\n",
    "df_filtered['Year'] = df_filtered['Date'].dt.year\n",
    "\n",
    "# Create Weekend column\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "df['Weekday'] = df['Date'].dt.day_name()\n",
    "df['Weekday'] = pd.Categorical(df['Weekday'], categories=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], ordered=True)\n",
    "df['Weekend'] = df['Weekday'].map(lambda x: 1 if x in ['Saturday', 'Sunday'] else 0)\n",
    "df.drop('Weekday', axis=1, inplace=True)\n",
    "\n",
    "# Drop the date column\n",
    "\n",
    "df_filtered.drop(['Date'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulting dataframe\n",
    "\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the graphs between all the columns of the dataframe, this allows us to have a first view of the correlations of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the correlation matrix\n",
    "\n",
    "numerical_columns = df_filtered.select_dtypes(include=[np.number]).columns\n",
    "corr_matrix = df_filtered[numerical_columns].corr()\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dew point temperature and temperature are very highly correlated (0.91), so we can drop one of them\n",
    "\n",
    "df_filtered.drop(['Dew point temperature(°C)'], axis=1, inplace=True)\n",
    "\n",
    "plt.scatter(df['Temperature(°C)'], df['Dew point temperature(°C)'])\n",
    "plt.xlabel('Temperature(°C)')\n",
    "plt.ylabel('Dew point temperature(°C)')\n",
    "plt.title('Scatter Plot of Temperature vs. Dew Point Temperature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df.apply(LabelEncoder().fit_transform)\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.xticks(rotation=90) \n",
    "sns.boxplot(data=df_encoded, orient='h') \n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see through this the variance of features.\n",
    "- There a few outliers, and they are mainly in the features Rainfall and Snowfall, so treating them would lead to removing those column's value, which isnt good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Rented Bike Count VS others features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Weekend', y='Rented Bike Count', data=df_encoded, ci=None)\n",
    "plt.title('Average Rented Bikes on Weekdays vs Weekends')\n",
    "plt.xlabel('Day Type')\n",
    "plt.ylabel('Average Rented Bike Count')\n",
    "plt.xticks([0, 1], ['Weekday', 'Weekend'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the difference in bikes rented between weekends and weekdays is very small so we can remove weekends from the data (\"df_encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the weekend column\n",
    "\n",
    "df_encoded.drop(['Weekend'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.cm.viridis(df['Rented Bike Count'] / max(df['Rented Bike Count']))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars = plt.bar(df['Hour'], df['Rented Bike Count'], color=colors)\n",
    "\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Rented Bike Count')\n",
    "plt.title('Rented Bike Count Per Hour')\n",
    "plt.xticks(range(0, 24))\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=df['Rented Bike Count'].min(), vmax=df['Rented Bike Count'].max()))\n",
    "cbar = plt.colorbar(sm, ax=plt.gca())\n",
    "cbar.set_label('Intensity')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_mapping = {1: 'Spring', 2: 'Summer', 3: 'Autumn', 4: 'Winter'}\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "graph = sns.lineplot(x='Hour', y='Rented Bike Count', hue='Seasons', data=df, marker='x', markeredgecolor='black')\n",
    "plt.xticks(range(0, 24))\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Rented Bike Count')\n",
    "plt.title('Hourly Distribution of Rented Bike Count for Each Season')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization shows that the values are arranged in the following order (in ascending order): Winter, Spring, Autumn, and Summer. This indicates that the demand for rented bikes tends to be lower during the winter season, followed by a gradual increase in the spring and autumn seasons. The highest values are observed during the summer season, possibly due to favorable weather conditions and increased outdoor activities. It seems that weather conditions affect the number of bike rentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest peak in bike rentals occurs between 4 PM and 8 PM (6 PM is the highest), indicating a significant demand during the evening hours. A notable spike is also observed at 8 AM, suggesting a demand for bikes during the morning rush hour.This suggests that many people use bikes to commute from home to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.copy()  \n",
    "df_temp['Holiday_Transformed'] = df_temp['Holiday'].map({\"No Holiday\": 0, \"Holiday\": 1})\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(df_temp[df_temp['Holiday_Transformed'] == 0]['Hour'], df_temp[df_temp['Holiday_Transformed'] == 0]['Rented Bike Count'], label='Normal Day', alpha=0.7)\n",
    "plt.scatter(df_temp[df_temp['Holiday_Transformed'] == 1]['Hour'], df_temp[df_temp['Holiday_Transformed'] == 1]['Rented Bike Count'], label='Holiday', alpha=0.7)\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Rented Bike Count')\n",
    "plt.title('Hourly Distribution of Rented Bike Count: Vacation vs Non-Vacation Days')\n",
    "plt.xticks(range(0, 24))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization reveals that bike rental counts are generally lower during vacation periods compared to non-vacation days. This suggests that the demand for rented bikes is influenced by holidays. The lower values during vacation periods may be attributed to a variety of factors, such as people not going to work, being away on trips, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "sns.scatterplot(x=df['Temperature(°C)'], y=df['Rented Bike Count'], alpha=0.5)\n",
    "plt.title('Scatter Plot: Temperature vs Rented Bike Count')\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Rented Bike Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rented_bikes_per_temp = df.groupby('Temperature(°C)')['Rented Bike Count'].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(avg_rented_bikes_per_temp['Temperature(°C)'], avg_rented_bikes_per_temp['Rented Bike Count'], marker='o')\n",
    "plt.title('Average Rented Bike Count per Temperature')\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Average Rented Bike Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last two visualizations wen can see that the demand for bikes decreases as the temperature drops. It reaches its highest point at around 30°C.\n",
    "\n",
    "For extremely high temperatures, a decrease in the number of rental bicycles is observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x='Temperature(°C)', y='Rented Bike Count', color='Seasons', hover_data=['Humidity(%)', 'Wind speed (m/s)'], size='Rented Bike Count')\n",
    "fig.update_layout(title='Overview of features')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualizations groups all we saw just before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.scatter(x=df['Humidity(%)'], y=df['Rented Bike Count'], alpha=0.5)\n",
    "plt.title('Humidity vs Rented Bike Count')\n",
    "plt.xlabel('Humidity')\n",
    "plt.ylabel('Rented Bike Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For extreme humidity values ​​the demand for bikes seems lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(df['Visibility (10m)'], df['Rented Bike Count'], alpha=0.5)\n",
    "plt.title('Visibility vs Rented Bike Count')\n",
    "plt.xlabel('Visibility (10m)')\n",
    "plt.ylabel('Rented Bike Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite naturally, the lower the visibility, the lower the demand for bikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "\n",
    "axes[0].scatter(df['Rainfall(mm)'], df['Rented Bike Count'])\n",
    "axes[0].set(xlabel='Rainfall (mm)', ylabel='Rented Bike Count', title='Rainfall vs Rented Bike Count')\n",
    "\n",
    "axes[1].scatter(df['Snowfall (cm)'], df['Rented Bike Count'])\n",
    "axes[1].set(xlabel='Snowfall (cm)', ylabel='Rented Bike Count', title='Snowfall vs Rented Bike Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both scatter plots exhibit a similar trend, showing that lower levels of rainfall and snowfall (approaching 0 cm) correspond to higher bike rental counts. This confirms that adverse weather conditions, such as heavy rain or snow, can act as deterrents for individuals to rent bikes. When faced with inclement weather, people may opt for alternative modes of transportation activities instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regulating skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare the skewness of the original data with the skewness of the transformed data\n",
    "# Goal: to find the transformation that makes the data the most normal aka the least skewed\n",
    "\n",
    "def compare_skew(df, column):\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20,5))\n",
    "    sns.distplot((df[column]), ax=axes[0]).set_title(\"Base data\")\n",
    "    sns.distplot(np.log1p(df[column]), ax=axes[1]).set_title(\"log1p\")\n",
    "    sns.distplot(np.sqrt(df[column]), ax=axes[2]).set_title(\"Square root\")\n",
    "    sns.distplot(np.cbrt(df[column]), ax=axes[3]).set_title(\"cube root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Rental Bike Count, we have a high skewness and kurtosis, so we will apply all 3 common transformations to see which one is the best\n",
    "compare_skew(df_encoded, 'Rented Bike Count')\n",
    "\n",
    "# There is a debate between the cube root and the square root transformation, but we will go with the square root\n",
    "df_encoded['Rented Bike Count'] = np.sqrt(df_encoded['Rented Bike Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_skew(df_encoded, 'Wind speed (m/s)')\n",
    "\n",
    "# The square root transformation is the best here\n",
    "df_encoded['Wind speed (m/s)'] = np.sqrt(df_encoded['Wind speed (m/s)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_skew(df_encoded, 'Visibility (10m)')\n",
    "\n",
    "# No transformation can help here, which was to be expected\n",
    "# We can expect the same thing from the rest of the columns with high skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded.drop(['Rented Bike Count'], axis=1)\n",
    "y = df_encoded['Rented Bike Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining train and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'X_train = {X_train.shape}, X_test = {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are trying to predict a continuous variable, so we will use regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the performance of the models\n",
    "\n",
    "results = pd.DataFrame(columns=['Model', 'R2', 'RMSE', 'MAE'])\n",
    "\n",
    "def train_and_evaluate(model, name, scaler=None):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    if scaler:\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(name)\n",
    "    print(f'R2: {r2}')\n",
    "    print(f'RMSE: {rmse}')\n",
    "    print(f'MAE: {mae}')\n",
    "\n",
    "    global results\n",
    "    if name not in results['Model'].values:\n",
    "        results = pd.concat([results, pd.DataFrame([[name, r2, rmse, mae]], columns=['Model', 'R2', 'RMSE', 'MAE'])])\n",
    "\n",
    "    plt.scatter(y_pred,y_test)\n",
    "    plt.xlim(0, 40)\n",
    "    plt.ylim(0, 40)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(LinearRegression(), 'Linear Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(DecisionTreeRegressor(), 'Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
    "train_and_evaluate(RandomForestRegressor(random_state=42, **rf_params), 'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "#GridSearchCV\n",
    "rf_grid_search = GridSearchCV(rf_regressor, rf_param_grid, scoring='r2', cv=3)\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "#best parameters\n",
    "print(\"Best Hyperparameters for Random Forest:\", rf_grid_search.best_params_)\n",
    "\n",
    "#Retrain the model with the best parameters\n",
    "best_rf_regressor = RandomForestRegressor(random_state=42, **rf_grid_search.best_params_)\n",
    "train_and_evaluate(best_rf_regressor, 'Random Forest with Best Parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(LGBMRegressor(random_state=42), 'LGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "lgbm = LGBMRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(lgbm, param_grid, scoring='r2', cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "#best parameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "#Retrain the model with the best parameters\n",
    "best_lgbm = LGBMRegressor(random_state=42, **grid_search.best_params_)\n",
    "train_and_evaluate(best_lgbm, 'LGBM with Best Parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 200, 'subsample': 0.8}\n",
    "\n",
    "train_and_evaluate(LGBMRegressor(random_state=42, **lgbm_params), 'LGBM SS', scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(LGBMRegressor(random_state=42, **lgbm_params), 'LGBM MMS', scaler=MinMaxScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(LGBMRegressor(random_state=42, **lgbm_params), 'LGBM RS', scaler=RobustScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(SVR(), 'SVR SS', scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(Lasso(), 'Lasso SS', scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(Ridge(), 'Ridge SS', scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_params = {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 1.0}\n",
    "\n",
    "train_and_evaluate(GradientBoostingRegressor(random_state=42, **grad_params), 'Gradient Boosting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "gb_regressor = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "gb_grid_search = GridSearchCV(gb_regressor, gb_param_grid, scoring='r2', cv=3)\n",
    "gb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters for Gradient Boosting with R2:\", gb_grid_search.best_params_)\n",
    "\n",
    "best_gb_regressor_r2 = GradientBoostingRegressor(random_state=42, **gb_grid_search.best_params_)\n",
    "train_and_evaluate(best_gb_regressor_r2, 'Gradient Boosting with Best Parameters (R2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(KNeighborsRegressor(), 'KNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt_params = {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
    "\n",
    "train_and_evaluate(ExtraTreesRegressor(random_state=42, **xt_params), 'Extra Trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "et_regressor = ExtraTreesRegressor(random_state=42)\n",
    "\n",
    "et_grid_search = GridSearchCV(et_regressor, et_param_grid, scoring='r2', cv=3)\n",
    "et_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters for Extra Trees:\", et_grid_search.best_params_)\n",
    "\n",
    "best_et_regressor = ExtraTreesRegressor(random_state=42, **et_grid_search.best_params_)\n",
    "train_and_evaluate(best_et_regressor, 'Extra Trees with Best Parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBRegressor\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "train_and_evaluate(XGBRegressor(random_state=42), 'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'subsample': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "xgb_regressor = XGBRegressor(random_state=42)\n",
    "\n",
    "xgb_grid_search = GridSearchCV(xgb_regressor, xgb_param_grid, scoring='r2', cv=3, verbose=2)\n",
    "xgb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters for XGBoost with R2:\", xgb_grid_search.best_params_)\n",
    "\n",
    "best_xgb_regressor_r2 = XGBRegressor(random_state=42, **xgb_grid_search.best_params_)\n",
    "train_and_evaluate(best_xgb_regressor_r2, 'XGBoost with Best Parameters (R2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 200, 'subsample': 0.8}\n",
    "train_and_evaluate(XGBRegressor(random_state=42, **xgb_params), 'XGBoost GSCV RS', scaler=RobustScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 200, 'subsample': 0.8}\n",
    "train_and_evaluate(XGBRegressor(random_state=42, **xgb_params), 'XGBoost GSCV SS', scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 200, 'subsample': 0.8}\n",
    "train_and_evaluate(XGBRegressor(random_state=42, **xgb_params), 'XGBoost GSCV MMS', scaler=MinMaxScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results.sort_values(by='R2', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
